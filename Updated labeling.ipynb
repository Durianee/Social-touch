{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workflow:\n",
    "\n",
    "Load the ResNet model from models.py.\n",
    "\n",
    "Extract frames from videos at a specified rate.\n",
    "\n",
    "Use the model to predict labels for these frames.\n",
    "\n",
    "Map the predictions to the labels provided in your text files (category_momentsv2.txt and category_multi_momentsv2.txt).\n",
    "\n",
    "Compile the results into a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Download and Run the model.py first (from github)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. import model class (resnet50), load model and process video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/006.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/006.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/009.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/009.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/021.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/021.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/023.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/023.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/031.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/032.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/031.mp4\n",
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/032.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/034.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/034.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/036.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/036.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/040.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/040.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/046.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/046.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/053.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/053.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/058.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: Could not open /Users/wangjiji/Desktop/Social touch deep learning project/STMP4/058.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/wangjiji/Desktop/moments_models-2')  # Replace with the path to your models.py\n",
    "from models import resnet50\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = resnet50(pretrained=True)  # Ensure this matches your specific function to load resnet50\n",
    "model.eval()\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                # Convert numpy array to PIL Image\n",
    "    transforms.Resize((224, 224)),          # Resize the image to 224x224\n",
    "    transforms.ToTensor(),                  # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Label mapping from your text file (adjust path as necessary)\n",
    "with open('category_momentsv2.txt', 'r') as f:\n",
    "    labels = {i: label.strip() for i, label in enumerate(f.readlines())}\n",
    "\n",
    "# Directory containing the videos\n",
    "directory = '/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/'\n",
    "\n",
    "# Prepare to collect results\n",
    "results = []\n",
    "\n",
    "# Process each video file from 001 to 071\n",
    "for i in range(1, 72):  # Adjust range for your video numbers\n",
    "    video_number = f\"{i:03}\"\n",
    "    video_path = f\"{directory}{video_number}.mp4\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Skipping: Could not open {video_path}\")\n",
    "        continue  # Skip this video file\n",
    "\n",
    "    frame_rate = 16\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = max(int(frame_count / frame_rate), 1)  # Prevents zero division\n",
    "\n",
    "    for frame_id in range(0, frame_count, interval):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        \n",
    "        # Apply transformations and predict\n",
    "        tensor = transform(frame)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tensor.unsqueeze(0))\n",
    "            predicted_label = labels[torch.argmax(outputs).item()]\n",
    "\n",
    "        results.append([video_number, frame_id, predicted_label])\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Save results to CSV\n",
    "# Creating the initial DataFrame\n",
    "df = pd.DataFrame(results, columns=['Video', 'Frame', 'Label'])\n",
    "\n",
    "# Pivoting the DataFrame\n",
    "pivot_df = df.pivot(index='Video', columns='Frame', values='Label')\n",
    "\n",
    "# Saving the pivoted DataFrame to CSV\n",
    "pivot_df.to_csv('/Users/wangjiji/Desktop/video_labels.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try resnet3D50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blow is the full code of \"models.py\", in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3x3 convolution with padding.\"\"\"\n",
    "    return nn.Conv3d(\n",
    "        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "def downsample_basic_block(x, planes, stride):\n",
    "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "    zero_pads = torch.Tensor(\n",
    "        out.size(0), planes - out.size(1),\n",
    "        out.size(2), out.size(3), out.size(4)).zero_()\n",
    "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "        zero_pads = zero_pads.cuda()\n",
    "    out = torch.cat([out.data, zero_pads], dim=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    Conv3d = staticmethod(conv3x3x3)\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = self.Conv3d(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = self.Conv3d(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    Conv3d = nn.Conv3d\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = self.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = self.Conv3d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = self.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet3D(nn.Module):\n",
    "\n",
    "    Conv3d = nn.Conv3d\n",
    "\n",
    "    def __init__(self, block, layers, shortcut_type='B', num_classes=305):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.conv1 = self.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], shortcut_type, stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], shortcut_type, stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], shortcut_type, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(\n",
    "                    downsample_basic_block,\n",
    "                    planes=planes * block.expansion,\n",
    "                    stride=stride,\n",
    "                )\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    self.Conv3d(\n",
    "                        self.inplanes,\n",
    "                        planes * block.expansion,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                    nn.BatchNorm3d(planes * block.expansion),\n",
    "                )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, self.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def modify_resnets(model):\n",
    "    # Modify attributs\n",
    "    model.last_linear, model.fc = model.fc, None\n",
    "\n",
    "    def features(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, features):\n",
    "        x = self.avgpool(features)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    # Modify methods\n",
    "    setattr(model.__class__, 'features', features)\n",
    "    setattr(model.__class__, 'logits', logits)\n",
    "    setattr(model.__class__, 'forward', forward)\n",
    "    return model\n",
    "\n",
    "\n",
    "ROOT_URL = 'http://moments.csail.mit.edu/moments_models'\n",
    "weights = {\n",
    "    'resnet50': 'moments_v2_RGB_resnet50_imagenetpretrained.pth.tar',\n",
    "    'resnet3d50': 'moments_v2_RGB_imagenet_resnet3d50_segment16.pth.tar',\n",
    "    'multi_resnet3d50': 'multi_moments_v2_RGB_imagenet_resnet3d50_segment16.pth.tar',\n",
    "}\n",
    "\n",
    "\n",
    "def load_checkpoint(weight_file):\n",
    "    if not os.access(weight_file, os.W_OK):\n",
    "        weight_url = os.path.join(ROOT_URL, weight_file)\n",
    "        os.system('wget ' + weight_url)\n",
    "    checkpoint = torch.load(weight_file, map_location=lambda storage, loc: storage)  # Load on cpu\n",
    "    return {str.replace(str(k), 'module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
    "\n",
    "\n",
    "def resnet50(num_classes=305, pretrained=True):\n",
    "    model = models.__dict__['resnet50'](num_classes=num_classes)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_checkpoint(weights['resnet50']))\n",
    "    model = modify_resnets(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet3d50(num_classes=305, pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet3D-50 model.\"\"\"\n",
    "    model = modify_resnets(ResNet3D(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, **kwargs))\n",
    "    if pretrained:\n",
    "         model.load_state_dict(load_checkpoint(weights['resnet3d50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def multi_resnet3d50(num_classes=292, pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet3D-50 model.\"\"\"\n",
    "    model = modify_resnets(ResNet3D(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, **kwargs))\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_checkpoint(weights['multi_resnet3d50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(arch):\n",
    "    model = {'resnet3d50': resnet3d50,\n",
    "             'multi_resnet3d50': multi_resnet3d50, 'resnet50': resnet50}.get(arch, 'resnet3d50')()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_transform():\n",
    "    \"\"\"Load the image transformer.\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "def load_categories(filename):\n",
    "    \"\"\"Load categories.\"\"\"\n",
    "    with open(filename) as f:\n",
    "        return [line.rstrip() for line in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/006.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/009.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/021.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/023.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/031.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/032.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/034.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/036.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/040.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/046.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/053.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/058.mp4\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.chdir('/Users/wangjiji/Desktop/moments_models-2')  # Replace with the path to your models.py\n",
    "from models import resnet3d50\n",
    "\n",
    "# Initialize the model with pretrained weights\n",
    "model = resnet3d50(num_classes=305, pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the path to the label file\n",
    "label_file_path = '/Users/wangjiji/Desktop/moments_models-2/category_multi_momentsv2.txt'  # Adjust this path as necessary\n",
    "\n",
    "# Load label mapping from the text file\n",
    "with open(label_file_path, 'r') as f:\n",
    "    labels = {i: label.strip() for i, label in enumerate(f.readlines())}\n",
    "\n",
    "# Define the transformation and preprocessing\n",
    "def preprocess(frame):\n",
    "    # Assuming frame is a numpy array of shape (H, W, C)\n",
    "    return cv2.resize(frame, (112, 112))  # Resize frame to the expected input size of the model\n",
    "\n",
    "# Directory containing the videos\n",
    "directory = Path('/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/')\n",
    "results = []\n",
    "\n",
    "# Process each video file from 001 to 071\n",
    "for i in range(1, 72):  # Adjust range for your video numbers\n",
    "    video_number = f\"{i:03}\"\n",
    "    video_path = directory / f\"{video_number}.mp4\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = preprocess(frame)  # Preprocess the frame\n",
    "        frames.append(frame)\n",
    "        if len(frames) == 8:  # Collect 8 frames per input sequence\n",
    "            clip = np.stack(frames, axis=0)\n",
    "            clip = np.transpose(clip, (3, 0, 1, 2))  # Reorder dimensions to C, D, H, W\n",
    "            clip = torch.tensor(clip, dtype=torch.float32).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(clip)\n",
    "                predicted_label = labels[outputs.argmax(dim=1).item()]  # Use the labels dictionary for mapping\n",
    "\n",
    "            results.append([video_path.name, frame_count, predicted_label])\n",
    "            frames = []  # Reset frames for next batch\n",
    "            frame_count += 16  # Increment frame index\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Convert results to DataFrame and pivot\n",
    "df = pd.DataFrame(results, columns=['Video', 'Frame', 'Label'])\n",
    "pivot_df = df.pivot(index='Video', columns='Frame', values='Label')\n",
    "pivot_df.to_csv('/Users/wangjiji/Desktop/video_labels_resnet3D50.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we try multi_resnet3d50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/006.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/009.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/021.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/023.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/031.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/032.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/034.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/036.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/040.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/046.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/053.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/058.mp4\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from models import multi_resnet3d50  # Import the function for initializing the multi_resnet3d50 model\n",
    "\n",
    "# Initialize the model with pretrained weights\n",
    "model = multi_resnet3d50(num_classes=292, pretrained=True)  # Ensure that num_classes aligns with the output layer of your model\n",
    "model.eval()\n",
    "\n",
    "# Define the path to the label file\n",
    "label_file_path = '/Users/wangjiji/Desktop/moments_models-2/category_multi_momentsv2.txt'  # Adjust this path as necessary\n",
    "\n",
    "# Load label mapping from the text file\n",
    "with open(label_file_path, 'r') as f:\n",
    "    labels = {i: label.strip() for i, label in enumerate(f.readlines())}\n",
    "\n",
    "# Define the transformation and preprocessing\n",
    "def preprocess(frame):\n",
    "    # Assuming frame is a numpy array of shape (H, W, C)\n",
    "    return cv2.resize(frame, (112, 112))  # Resize frame to the expected input size of the model\n",
    "\n",
    "# Directory containing the videos\n",
    "directory = Path('/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/')\n",
    "results = []\n",
    "\n",
    "# Process each video file from 001 to 071\n",
    "for i in range(1, 72):  # Adjust range for your video numbers\n",
    "    video_number = f\"{i:03}\"\n",
    "    video_path = directory / f\"{video_number}.mp4\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = preprocess(frame)  # Preprocess the frame\n",
    "        frames.append(frame)\n",
    "        if len(frames) == 8:  # Collect 8 frames per input sequence\n",
    "            clip = np.stack(frames, axis=0)\n",
    "            clip = np.transpose(clip, (3, 0, 1, 2))  # Reorder dimensions to C, D, H, W\n",
    "            clip = torch.tensor(clip, dtype=torch.float32).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(clip)\n",
    "                predicted_label = labels[outputs.argmax(dim=1).item()]  # Use the labels dictionary for mapping\n",
    "\n",
    "            results.append([video_path.name, frame_count, predicted_label])\n",
    "            frames = []  # Reset frames for next batch\n",
    "            frame_count += 16  # Increment frame index\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Convert results to DataFrame and pivot\n",
    "df = pd.DataFrame(results, columns=['Video', 'Frame', 'Label'])\n",
    "pivot_df = df.pivot(index='Video', columns='Frame', values='Label')\n",
    "pivot_df.to_csv('/Users/wangjiji/Desktop/video_labels_multi_resnet3D50.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
