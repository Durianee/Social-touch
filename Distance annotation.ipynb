{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Firstly, you should download the model file \"pytorch-openpose-master\" to your local system, and then modify the relevant path information to ensure the code runs successfully.**\n",
    "\n",
    "you can find the master model file and the processed videos in  my Google Drive :\n",
    "\n",
    "**https://drive.google.com/drive/folders/1CgMIg8yLFkGJvWBV_nZk1m35ETliu7Ya?usp=drive_link**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet imports the `sys` module and appends a specified path to the system path. The path `/Users/wangjiji/Desktop/pytorch-openpose-master/model` should be adjusted to the location of the 'src' directory within the downloaded \"pytorch-openpose-master\" folder on your local system. This ensures that the Python interpreter can locate and import modules from this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/wangjiji/Desktop/pytorch-openpose-master/model')  # Adjust this path to the root where 'src' is located in the downloaded \"pytorch-openpose-master\" folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code that write video to visulise the distances between each skeleton keypoints\n",
    "\n",
    "**Note the runing time is really long so pls also modify the path of input folder for selected video processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from src.body import Body\n",
    "from src import util\n",
    "from tqdm import tqdm\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "# Initialize the Body pose model\n",
    "body_estimation = Body('/Users/wangjiji/Desktop/pytorch-openpose-master/model/body_pose_model.pth')\n",
    "\n",
    "def extract_keypoints(frame):\n",
    "    candidate, subset = body_estimation(frame)\n",
    "    return candidate, subset\n",
    "\n",
    "def process_frame(frame):\n",
    "    candidate, subset = extract_keypoints(frame)\n",
    "    canvas = frame.copy()\n",
    "    canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "    return canvas, candidate, subset\n",
    "\n",
    "def calculate_distances(candidates):\n",
    "    distances = []\n",
    "    for i, candidate1 in enumerate(candidates):\n",
    "        for j, candidate2 in enumerate(candidates):\n",
    "            if i < j:\n",
    "                distance = np.linalg.norm(candidate1[:2] - candidate2[:2])\n",
    "                distances.append((i, j, distance))\n",
    "    return distances\n",
    "\n",
    "def annotate_frame(frame, distances, candidates):\n",
    "    for (i, j, distance) in distances:\n",
    "        label = f\"Dist: {distance:.2f}\"\n",
    "        x1, y1 = int(candidates[i][0]), int(candidates[i][1])\n",
    "        x2, y2 = int(candidates[j][0]), int(candidates[j][1])\n",
    "        cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        mid_x, mid_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        cv2.putText(frame, label, (mid_x, mid_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return frame\n",
    "\n",
    "def process_video(video_file, output_file, output_type='video'):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Cannot open video file {video_file}\")\n",
    "        return\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    if output_type == 'video':\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_file, fourcc, fps, (width, height))\n",
    "    else:\n",
    "        frames = []\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    progress_bar = tqdm(total=frame_count, desc=f\"Processing {os.path.basename(video_file)}\", leave=False)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        processed_frame, candidates, subset = process_frame(frame)\n",
    "        distances = calculate_distances(candidates)\n",
    "        annotated_frame = annotate_frame(processed_frame, distances, candidates)\n",
    "        if output_type == 'video':\n",
    "            out.write(annotated_frame)\n",
    "        else:\n",
    "            frames.append(annotated_frame)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    if output_type == 'video':\n",
    "        out.release()\n",
    "    else:\n",
    "        clip = ImageSequenceClip([cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in frames], fps=30)\n",
    "        clip.write_gif(output_file)\n",
    "\n",
    "    progress_bar.close()\n",
    "    print(f\"Processed video has been written to {output_file}\")\n",
    "\n",
    "def main(input_folder, output_folder, output_type='video'):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    video_files = [os.path.join(input_folder, f\"{i:03}.mp4\") for i in range(1, 72)]\n",
    "    valid_video_files = [f for f in video_files if os.path.exists(f)]\n",
    "    \n",
    "    overall_progress = tqdm(total=len(valid_video_files), desc=\"Overall Progress\")\n",
    "\n",
    "    for video_file in valid_video_files:\n",
    "        if output_type == 'video':\n",
    "            output_file = os.path.join(output_folder, f\"{os.path.basename(video_file).replace('.mp4', '.mp4')}\")\n",
    "        else:\n",
    "            output_file = os.path.join(output_folder, f\"{os.path.basename(video_file).replace('.mp4', '.gif')}\")\n",
    "        \n",
    "        process_video(video_file, output_file, output_type)\n",
    "        overall_progress.update(1)\n",
    "    \n",
    "    overall_progress.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_folder = '/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/Openpose processed'  # Path to the input folder\n",
    "    output_folder = '/Users/wangjiji/Desktop/Social touch deep learning project/STMP4/Distance Annotated'  # Path to save the processed videos\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    user_input = input(\"Do you want to generate video or gif? (enter 'video' or 'gif'): \").strip().lower()\n",
    "    if user_input in ['video', 'gif']:\n",
    "        output_type = user_input\n",
    "    else:\n",
    "        output_type = 'video'  # Default to video if input is invalid\n",
    "\n",
    "    main(input_folder, output_folder, output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is certainly need to be adjusted, maybe we can add some function to detect the major movements between characters...to be modified.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
